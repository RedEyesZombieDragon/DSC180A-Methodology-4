# Akshay Medidi @amedidi@ucsd.edu

# Section A07 with Ryan Lingo

**1. What is the most interesting topic covered in your domain this quarter?**
So far, the most interesting topic we have covered in my domain was about the different strategies used for evaluating LLM outputs. Specifically, I did not know that using an LLM to judge LLM generated content is one of the ways that is currently being pushed and researched into as to make it good enough to compete with humans as judges.

**2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.**
Something cool that I read about while doing research was having an LLM answer a test and then grade the responses. In this paper, it turned out the model was better at doing the test than grading it's own responses. So in turn, for our project, we can have an LLM evaluate our LLM slide summaries, and then have another LLM evaluate the reasoning it used behind it's grading.

**3. What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**
Our current Quarter 1 Project is very barebones at the moment and does not have much actual reflection on LLM evaluation. So going forward, I would like for us to specifically put more emphasis and focus on the LLM evaluation side of the project rather than the LLM that creates the slide summaries.

**4. What other techniques would you be interested in using in your project?**
A technique I would be interested in using during my project would be some sort of website to be able to scale how we can get human evaluation data reasonably. I would like to have humans evaluate our LLM evaluators, and to do so, we would need an efficient way to get this data to lots of people. I believe a website, similar to LM arena could help us achieve this goal.
